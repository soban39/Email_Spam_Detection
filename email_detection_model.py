# -*- coding: utf-8 -*-
"""Email Detection Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vJfwtHXtb3-eB1yMhtvl4xS_zOy1pbzZ

Import Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score , confusion_matrix
from sklearn.metrics import classification_report
from nltk.corpus import stopwords
from collections import Counter

"""Data Acquisition:"""

df = pd.read_csv("spam.csv", encoding="ISO-8859-1")

df.head()

rem_col = ['Unnamed: 2','Unnamed: 3','Unnamed: 4']
df = df.drop(rem_col, axis=1)
df.head()

df.shape

df.info()

spam_count = df[df["v1"] == "spam"].shape[0]
ham_count = df[df["v1"] == "ham"].shape[0]
print(f"Spam emails: {spam_count}, Ham emails: {ham_count}")
print("Missing values:\n", df.isnull().sum())

df.describe()

df[df.duplicated()].head()

df = df.drop_duplicates()
df.head()

df["v1"].value_counts()

"""EDA (Exploratory Data Analysis):"""

sns.countplot(data=df, x='v1')
plt.xlabel('v1')
plt.ylabel('count')
plt.title('Count Plot')
plt.show()

from wordcloud import WordCloud

spam_words = ' '.join(list(df[df['v1'] == 'spam']['v2']))
ham_words = ' '.join(list(df[df['v1'] == 'ham']['v2']))

spam_cloud = WordCloud(width=500, height=300, max_font_size=100).generate(spam_words)
ham_cloud = WordCloud(width=500, height=300, max_font_size=100).generate(ham_words)

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(spam_cloud, interpolation='bilinear')
plt.title('Spam Emails')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(ham_cloud, interpolation='bilinear')
plt.title('Ham Emails')
plt.axis('off')

plt.show()

df.loc[df["v1"] == "spam", "Category"] = 0
df.loc[df["v1"] == "ham", "Category"] = 1

df.head()

x=df['v2']
y=df['Category']

x.head()

y.head()

"""Feature Engineering:"""

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

def preprocess_text(text):
    return ' '.join([stemmer.stem(word) for word in text.split()])

df['v2'] = df['v2'].apply(preprocess_text)

"""Model Training

Logistic Regression :
"""

X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)

feature_extraction = TfidfVectorizer(min_df=1, stop_words='english',lowercase=True)
X_train_features = feature_extraction.fit_transform(X_train)
X_test_features = feature_extraction.transform(X_test)

y_train = y_train.astype(int)
y_test = y_test.astype(int)

model = LogisticRegression()
model.fit(X_train_features,y_train)

prediction_on_training_data = model.predict(X_train_features)
accuracy_on_training_data = accuracy_score(y_train, prediction_on_training_data)

prediction_on_test_data = model.predict(X_test_features)
accuracy_on_test_data = accuracy_score(y_test, prediction_on_test_data)

# Print accuracy
print('Accuracy on training data: {} %'.format(accuracy_on_training_data * 100))
print('Accuracy on test data: {} %'.format(accuracy_on_test_data * 100))

conf_matrix = confusion_matrix(y_test, prediction_on_test_data)
plt.figure(figsize=(10, 8))  # Slightly larger figure
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="coolwarm", cbar=True,  # Changed color map and enabled color bar
            annot_kws={"size": 14, "weight": "bold"},  # Updated annotation styling
            xticklabels=['Spam', 'Ham'], yticklabels=['Spam', 'Ham'])
plt.xlabel('Predicted', fontsize=14, fontweight='bold')  # Modified label font style
plt.ylabel('Actual', fontsize=14, fontweight='bold')
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')  # Changed title style
plt.xticks(fontsize=12, rotation=45, ha='right')  # Adjusted tick label rotation and alignment
plt.yticks(fontsize=12)
plt.show()

classification_rep = classification_report(y_test, prediction_on_test_data,target_names=['Spam', 'Ham'])
print("Classification Report:")
print(classification_rep)

TP = conf_matrix[1, 1]
TN = conf_matrix[0, 0]
FP = conf_matrix[0, 1]
FN = conf_matrix[1, 0]

accuracy = (TP + TN) / (TP + TN + FP + FN)
precision = TP / (TP + FP)
recall = TP / (TP + FN)
specificity = TN / (TN + FP)

print("Accuracy : ",accuracy)
print("Precision : ",precision)
print("Recall : ",recall)
print("Specificity : ",specificity)

input_your_mail ="Dear John, I hope this email finds you well. Attached is the report you requested for our upcoming meeting. Please review it at your convenience"
input_data_features = feature_extraction.transform([input_your_mail])
prediction = model.predict(input_data_features)
if prediction[0] == 1:
    print("Ham Mail")
else:
    print("Spam Mail")

"""Random Forest Classifier:"""

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier()
rf_model.fit(X_train_features, y_train)

rf_prediction = rf_model.predict(X_test_features)
print("Random Forest Accuracy:", accuracy_score(y_test, rf_prediction))
print('Accuracy on training data: {} %'.format(accuracy_on_training_data * 100))
print('Accuracy on test data: {} %'.format(accuracy_on_test_data * 100))

"""Model Evaluation:"""

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_train_features, y_train, cv=5)
print("Cross-validation scores:", scores)
print("Average cross-validation score:", scores.mean())

"""Bonus Deep Dives:

Feature Engineering:
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

deep_model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=X_train_features.shape[1]),
    LSTM(64, return_sequences=True),
    LSTM(32),
    Dense(1, activation='sigmoid')
])
deep_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

from tensorflow.keras.preprocessing.text import Tokenizer


# Load dataset (adjust path if necessary)
df = pd.read_csv("spam.csv", encoding="ISO-8859-1")

# Preprocess the dataset
df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)
df.columns = ['label', 'text']
df['label'] = df['label'].map({'spam': 0, 'ham': 1})

# Split data into features and labels
X = df['text']
y = df['label']

# Initialize the tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

# Calculate Vocabulary Size
vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token
print(f"Vocabulary Size (input_dim): {vocab_size}")

# Calculate Maximum Sequence Length
sequence_lengths = [len(text.split()) for text in X]
maxlen = max(sequence_lengths)
print(f"Maximum Sequence Length (maxlen): {maxlen}")

# Calculate 95th Percentile Sequence Length (optional)
percentile_95 = int(np.percentile(sequence_lengths, 95))
print(f"95th Percentile Sequence Length: {percentile_95}")

from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize the text data
X_sequences = tokenizer.texts_to_sequences(X)

# Pad sequences to the maximum length (use maxlen or percentile_95)
X_padded = pad_sequences(X_sequences, maxlen=maxlen, padding='post')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

deep_model = Sequential([
    Embedding(input_dim=8921, output_dim=128, input_length=maxlen),  # Updated input_dim and maxlen
    LSTM(64, return_sequences=True),
    LSTM(32),
    Dense(1, activation='sigmoid')
])

deep_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""Model Training:"""

deep_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

"""Evaluation:"""

test_loss, test_accuracy = deep_model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")